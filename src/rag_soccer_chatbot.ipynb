{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proyecto #2 - Soccer Chatbot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fine Tunning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to fine tune the Llama3 model with some pdfs from our knowledge database to improve its responses in terms of quality of the response and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to load the pdf in memory to work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "pdf_path = os.path.abspath('../docs/knowledge-database/documents/The ball is round.pdf')\n",
    "\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the pdf in memory, we can manipulate its contents to use them in a correct way.\n",
    "\n",
    "We have to select the relevant pages. In this document the pages that contain relevant information are the pages from page 23 to page 987."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[22:987]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have clean the data to remove irrelevant characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import roman\n",
    "\n",
    "# Function to check if a string is a roman number\n",
    "def isRomanNumeral(s):\n",
    "    try:\n",
    "        roman.fromRoman(s)\n",
    "        return True\n",
    "    except roman.InvalidRomanNumeralError:\n",
    "        return False\n",
    "\n",
    "# Extract page contents\n",
    "def extractPageContents(data):\n",
    "    pages = []\n",
    "    for page in data:\n",
    "        pages += [page.page_content]\n",
    "\n",
    "    return pages\n",
    "\n",
    "# Split pages by lines\n",
    "def splitPagesIntoLines(pages):\n",
    "    lines = []\n",
    "    for page in pages:\n",
    "        lines += page.split('\\n')\n",
    "\n",
    "    return lines\n",
    "\n",
    "# Clean the lines\n",
    "def cleanLines(lines):\n",
    "    cleanedLines = []\n",
    "    for line in lines:\n",
    "        temp = line.strip()\n",
    "        \n",
    "        if (temp.isdigit()):\n",
    "            continue\n",
    "        elif (temp == ''):\n",
    "            continue\n",
    "        elif (isRomanNumeral(temp)):\n",
    "            continue\n",
    "\n",
    "        temp = re.sub(r\"’\\d\", \"’\", temp)\n",
    "        temp = re.sub(r\"\\.\\d\", \".\", temp)\n",
    "        \n",
    "        cleanedLines += [temp]\n",
    "    \n",
    "    return cleanedLines\n",
    "\n",
    "pages = extractPageContents(data)\n",
    "lines = splitPagesIntoLines(pages)\n",
    "cleanedLines = cleanLines(lines)\n",
    "\n",
    "cleanedLines\n",
    "\n",
    "# Merge all the lines into a single string\n",
    "cleanedText = ' '.join(cleanedLines)\n",
    "# print(cleanedText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the text is cleaned, we can tokenize it. In this case we are going to tokenize it into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "sentences = nltk.sent_tokenize(cleanedText)\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still some junk chars, so we have to clean them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedSentences = []\n",
    "for sentece in sentences:\n",
    "    if (sentece != '.'):\n",
    "        cleanedSentences += [sentece]\n",
    "\n",
    "cleanedSentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have cleaned the data, we can prepare our dataset to fine tune the model.\n",
    "\n",
    "First we have to label the sentences. In this case we are going to label a sentence with the next sentence. Also, we are going to create a Dataset object, so it can be processed by PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataDict = {\n",
    "    'inputText': cleanedSentences ,\n",
    "    'targetText': cleanedSentences[1:] + [None]\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(dataDict)\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bosonai/Higgs-Llama-3-70B')\n",
    "\n",
    "def tokenizeFunction(data):\n",
    "    return tokenizer(data['inputText'], return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "\n",
    "tokenizedDataset = dataset.map(tokenizeFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, LlamaForConditionalGeneration\n",
    "\n",
    "model = LlamaForConditionalGeneration.from_pretrained('llama')\n",
    "\n",
    "trainingArgs = TrainingArguments(\n",
    "    output_dir='./fine-tuned-model',\n",
    "    num_train_epochs=3,\n",
    "    per_device_eval_batch_size=8,\n",
    "    save_steps=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=trainingArgs,\n",
    "    train_dataset=tokenizedDataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = 'bert-base-uncased'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# encodedInput = tokenizer(sentences, padding='max_length', truncation=True)\n",
    "\n",
    "# inputsIds = encodedInput['input_ids']\n",
    "# attentionMask = encodedInput['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to create a dataset, so pytorch can process it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Results**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

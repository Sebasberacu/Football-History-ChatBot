{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proyecto #2 - Soccer Chatbot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fine Tunning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to fine tune the Llama3 model with some pdfs from our knowledge database to improve its responses in terms of quality of the response and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to load the pdf in memory to work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "pdf_path = os.path.abspath('../docs/knowledge-database/documents/The ball is round.pdf')\n",
    "\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the pdf in memory, we can manipulate its contents to use them in a correct way.\n",
    "\n",
    "We have to select the relevant pages. In this document the pages that contain relevant information are the pages from page 23 to page 987."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[22:987]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have clean the data to remove irrelevant characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import roman\n",
    "\n",
    "# Function to check if a string is a roman number\n",
    "def is_roman_numeral(s):\n",
    "    try:\n",
    "        roman.fromRoman(s)\n",
    "        return True\n",
    "    except roman.InvalidRomanNumeralError:\n",
    "        return False\n",
    "\n",
    "# Extract page contents\n",
    "pages = []\n",
    "for page in data:\n",
    "    pages  += [page.page_content]\n",
    "\n",
    "# Split data by newlines\n",
    "lines = []\n",
    "for page in pages:\n",
    "    lines += page.split('\\n')\n",
    "\n",
    "# Clean the lines\n",
    "cleaned_lines = []\n",
    "for line in lines:\n",
    "    temp = line.strip()\n",
    "    \n",
    "    if (temp.isdigit()):\n",
    "        continue\n",
    "    elif (temp == ''):\n",
    "        continue\n",
    "    elif (is_roman_numeral(temp)):\n",
    "        continue\n",
    "\n",
    "    temp = re.sub(r\"’\\d\", \"’\", temp)\n",
    "    temp = re.sub(r\"\\.\\d\", \".\", temp)\n",
    "    \n",
    "    cleaned_lines += [temp]\n",
    "\n",
    "cleaned_lines\n",
    "\n",
    "# Merge all the lines into a single string\n",
    "cleaned_text = ' '.join(cleaned_lines)\n",
    "# print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the text is cleaned, we can tokenize it. In this case we are going to tokenize it into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "sentences = nltk.sent_tokenize(cleaned_text)\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to tokenize each sentence, so we can use them to fine tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# model = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# tokenized_sentences = tokenizer(sentences, padding='max_length', truncation=True)\n",
    "\n",
    "# tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to label the sentences, in this case we are going to label the sentence with the next sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sentence_pairs = []\n",
    "for i in range(len(sentences) - 1):\n",
    "    sentence_pairs.append({\n",
    "        'sentence': sentences[i],\n",
    "        'label': sentences[i + 1]\n",
    "    })\n",
    "\n",
    "sentence_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(sentence_pairs)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Results**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
